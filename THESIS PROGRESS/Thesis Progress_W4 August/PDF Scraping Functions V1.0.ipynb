{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde5c60a",
   "metadata": {},
   "source": [
    "# Modules for PDF Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f63382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Bobby A\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "#PDF Miner (PDF Scraping)\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import HTMLConverter, TextConverter, XMLConverter\n",
    "from io import StringIO\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "\n",
    "#PyPDF2 (PDF Scraping)\n",
    "import PyPDF2 as pypdf\n",
    "\n",
    "\n",
    "#OCR\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd=r'Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "\n",
    "#NLP Preprocessing\n",
    "import nltk\n",
    "import re\n",
    "import regex\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "#NLP Processing TensorFLow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "#NLP Processing Spacy\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "spc = spacy.load('en_core_web_trf')\n",
    "# spc = spacy.load(en_core_web_lg)\n",
    "# spc.add_pipe('sentencizer')\n",
    "\n",
    "#Model Training and Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import docker\n",
    "import string\n",
    "\n",
    "pc = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adb8b6",
   "metadata": {},
   "source": [
    "# Functions for PDF Text Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033442",
   "metadata": {},
   "source": [
    "## PDF-Miner (Unscanned PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb79a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_file_content_Text(path_to_pdf):\n",
    "    '''\n",
    "    path_to_pdf: is the parameter that will give access to the PDF File \n",
    "    we want to extract the content.\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    PDFResourceManager is used to store shared resources such as fonts or images that \n",
    "    we might encounter in the files. \n",
    "    '''\n",
    "\n",
    "    resource_manager = PDFResourceManager(caching=True)\n",
    "\n",
    "    '''\n",
    "    create a string object that will contain the final text the representation of the pdf. \n",
    "    '''\n",
    "    out_text = StringIO()\n",
    "\n",
    "    '''\n",
    "    UTF-8 is one of the most commonly used encodings, and Python often defaults to using it.\n",
    "    In our case, we are going to specify in order to avoid some encoding errors.\n",
    "    '''\n",
    "    codec = 'utf-8'\n",
    "\n",
    "    \"\"\"\n",
    "    LAParams is the object containing the Layout parameters with a certain default value. \n",
    "    \"\"\"\n",
    "    laParams = LAParams(line_overlap=0.3, detect_vertical=True)\n",
    "\n",
    "    '''\n",
    "    Create a TextConverter Object, taking :\n",
    "    - ressource_manager,\n",
    "    - out_text \n",
    "    - layout parameters.\n",
    "    '''\n",
    "    text_converter = TextConverter(resource_manager, out_text, laparams=laParams)\n",
    "    fp = open(path_to_pdf, 'rb')\n",
    "\n",
    "    '''\n",
    "    Create a PDF interpreter object taking: \n",
    "    - ressource_manager \n",
    "    - text_converter\n",
    "    '''\n",
    "    interpreter = PDFPageInterpreter(resource_manager, text_converter)\n",
    "\n",
    "    '''\n",
    "    We are going to process the content of each page of the original PDF File\n",
    "    '''\n",
    "    try:\n",
    "        for page in PDFPage.get_pages(fp, pagenos=set(), maxpages=0, password=\"\", caching=True, check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "    except:\n",
    "        for page in PDFPage.get_pages(fp, pagenos=set(), maxpages=0, password=b\"\", caching=True, check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "    '''\n",
    "    Retrieve the entire contents of the “file” at any time \n",
    "    before the StringIO object’s close() method is called.\n",
    "    '''\n",
    "    text = out_text.getvalue()\n",
    "\n",
    "    '''\n",
    "    Closing all the ressources we previously opened\n",
    "    '''\n",
    "    fp.close()\n",
    "    text_converter.close()\n",
    "    out_text.close()\n",
    "\n",
    "    '''\n",
    "    Return the final variable containing all the text of the PDF\n",
    "    '''\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b20f9",
   "metadata": {},
   "source": [
    "## Group PDF Scraping for normal method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61eb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_read_pdfminer(folder_path):\n",
    "    all_extracted_text={}\n",
    "    for i,filename in enumerate(glob.glob(os.path.join(folder_path, '*.pdf'))):\n",
    "        try:\n",
    "            all_extracted_text[re.search(r'(\\w+).pdf$',filename).group(1) +'_'+ str(i)] = get_pdf_file_content_Text(filename).lower()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591bf3c2",
   "metadata": {},
   "source": [
    "## OCR Tesseract (Scanned Document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_file_content_Text_ocr(pdf_path)\n",
    "    #Convert pdf to images\n",
    "    images = convert_from_path(pdf_path, 500,poppler_path=r'C:\\Program Files\\poppler-0.68.0\\bin')\n",
    "    output_path = 'ocr_out/'\n",
    "    for i, image in enumerate(images):\n",
    "        fname = output_path + 'image_'+str(i)+'.png'\n",
    "        image.save(fname, \"PNG\")\n",
    "\n",
    "    extracted_text = []\n",
    "    #images to text\n",
    "    for i,filename in enumerate(glob.glob(os.path.join(output_path, '*.png'))):\n",
    "        try:\n",
    "            img1 = cv2.imread(filename)\n",
    "            extracted_text.append(pytesseract.image_to_string(img1))\n",
    "            print('successful'+str(i))\n",
    "        except:\n",
    "            print('unsuccessful'+str(i))\n",
    "\n",
    "    text = ' '.join(extracted_text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc92b5",
   "metadata": {},
   "source": [
    "## Group PDF Scraping for OCR method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60daa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_read_ocr(folder_path):\n",
    "    all_extracted_text={}\n",
    "    for i,filename in enumerate(glob.glob(os.path.join(folder_path, '*.pdf'))):\n",
    "        try:\n",
    "            all_extracted_text[re.search(r'(\\w+).pdf$',filename).group(1) +'_'+ str(i)] = get_pdf_file_content_Text_ocr(filename).lower()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95afb32",
   "metadata": {},
   "source": [
    "# Function for Preprocessing 1\n",
    "### Regex Conditioning for Scientific Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(text_pdfmine):\n",
    "    #Punctuation of '-' because make a newline \n",
    "    text_punew = re.sub(r'(-\\n?\\s)','',text_pdfmine.lower())\n",
    "    \n",
    "    #Remove spacing n dot '\\n' & '•'\n",
    "    if bool(regex.search(r'(?<=(\\w\\w))[\\n•](?=(\\w\\w))',text_punew))==True:\n",
    "        text_punew_spdo = re.sub(r'[\\n•]', ' ', text_punew)\n",
    "    else:\n",
    "        text_punew_spdo = re.sub(r'[\\n•]', '', text_punew)\n",
    "        \n",
    "    #Adjustment in Germany alphabetic\n",
    "    text_punew_spdo_ger = re.sub(r'(?<=[oua])(¨)','e',text_punew_spdo)\n",
    "    text_punew_spdo_ger = re.sub(r'ß','ss',text_punew_spdo_ger)\n",
    "    \n",
    "    #Adjusment in Over-Spacing\n",
    "    text_punew_spdo_ger_ovsp = re.sub(r'(\\s)(?=\\s+)','',text_punew_spdo_ger)\n",
    "    \n",
    "    #Delete et al. + lower\n",
    "    text_punew_spdo_ger_ovsp_etal = regex.sub(r'(\\w+\\s+et al.)','',text_punew_spdo_ger_ovsp)\n",
    "    \n",
    "    #Eliminate fig and table\n",
    "    text_punew_spdo_ger_ovsp_etal = re.sub(r'ﬁ','fi',text_punew_spdo_ger_ovsp_etal)\n",
    "    text_punew_spdo_ger_ovsp_etal_fig = re.sub(r'(table|tables|figure|figures|fig.?|figs.)\\s+(\\d+|\\d+.)', '', text_punew_spdo_ger_ovsp_etal)\n",
    "    \n",
    "    #Eliminate website\n",
    "    text_punew_spdo_ger_ovsp_etal_fig_emai_web = regex.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w\\s_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?','',text_punew_spdo_ger_ovsp_etal_fig_emai)\n",
    "    \n",
    "    return text_punew_spdo_ger_ovsp_etal_fig_emai_web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881584c7",
   "metadata": {},
   "source": [
    "# Function for Preprocessing 2 \n",
    "### (Sentences seperation (by SpaCy) with Conditioning, Stemmer / Lemmatizer, & Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7fd624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_2(text_punew_spdo_ger_ovsp_etal_fig_emai_web):\n",
    "    #Splitted into sentences by Spacy\n",
    "    spc_text = spc(text_punew_spdo_ger_ovsp_etal_fig_emai_web)\n",
    "\n",
    "    #Preparation\n",
    "    prepro1 = []\n",
    "    col_intro = []\n",
    "    stop_ack_stc = []\n",
    "    stop_ref_stc = []\n",
    "    name = []\n",
    "    prepro2=[]\n",
    "    \n",
    "    #Read the file Stopwords\n",
    "    gist_file = open(\"gist_stopwords.txt\", \"r\")\n",
    "    try:\n",
    "        content = gist_file.read()\n",
    "        stopwords = content.split(\",\")\n",
    "    finally:\n",
    "        gist_file.close()\n",
    "\n",
    "    #List all possible name with spaCy\n",
    "    for i in spc_text.ents:\n",
    "        if i.label_.lower() == 'person':\n",
    "            name.append(i.text)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #Pattern for combination of alphabet and digit in word\n",
    "    pca = '!\"#$%&\\*+/:;<=>?@[\\]^_`{|}~'\n",
    "    pattern_comb = regex.compile(r'([a-z]+[\\d]+|\\d+[a-z]+|\\d+[{}]+|[a-z]+[{}]+|[{}]+[a-z]+|[{}]+[\\d]+)'.format(pca,pca,pca,pca))\n",
    "    \n",
    "    #List of Sentences with application of lemmatization\n",
    "    for i,j in enumerate(spc_text.sents):\n",
    "\n",
    "        #Eliminate word - number - punc-chars combination\n",
    "        sent_lem = regex.sub(pattern_comb,'',j.lemma_)\n",
    "\n",
    "        #delete name\n",
    "        if len(name) != 0:\n",
    "            try:\n",
    "                sent_lem = regex.sub(r\",*(\\s*\\b(?:{}))\\b\".format(\"|\".join(name)),'',sent_lem)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        prepro1.append(sent_lem)\n",
    "\n",
    "        #Search for the start pf acknowledgements and references\n",
    "        if bool(re.search('(acknowledgments|acknowledgment)',sent_lem)) == True:\n",
    "            stop_ack_stc.append(i)\n",
    "\n",
    "        if bool(re.search('(references|reference)',sent_lem)) == True:\n",
    "            stop_ref_stc.append(i)\n",
    "\n",
    "        elif bool(re.search('(introduction)',sent_lem)) == True:\n",
    "            col_intro.append(i)\n",
    "\n",
    "    #Cut parts before Introduction and after the acknowledgments or references\n",
    "    try:\n",
    "        if len(stop_ack_stc) == 0:\n",
    "            try:\n",
    "                prepro1 = prepro1[col_intro[0]:stop_ref_stc[-1]]\n",
    "            except:\n",
    "                prepro1 = prepro1[:stop_ref_stc[-1]]\n",
    "        else:\n",
    "            try:\n",
    "                prepro1 = prepro1[col_intro[0]:stop_ack_stc[-1]]\n",
    "            except:\n",
    "                prepro1 = prepro1[:stop_ack_stc[-1]]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #Pattern for Remove Citation\n",
    "    pattern1 = regex.compile(r'\\(([\\w\\s\\d{}]+)\\)'.format(pc))\n",
    "\n",
    "    #Pattern for Apply the Stopwords\n",
    "    pattern2 = r\",*(\\s*\\b(?:{}))\\b\".format(\"|\".join(stopwords))\n",
    "\n",
    "    #Pattern for unused space first n last\n",
    "    pattern3 = regex.compile(r'((^\\s+)(?=.)|(?<=.)(\\s+$))')\n",
    "\n",
    "    #Pattern for Apply over-space\n",
    "    pattern4 = regex.compile(r'(\\s)(?=\\s+)')\n",
    "\n",
    "    #Pattern Additional compiler\n",
    "    pattern_add = regex.compile(r'(\\x01|\\x0c|\\s[a-z]\\s|°|^\\b(\\w\\s)|I|)')\n",
    "\n",
    "    #Pattern for combined word and number\n",
    "    for i,j in enumerate(prepro1):\n",
    "        if bool(regex.search(pattern1,j)) == True:\n",
    "            if bool(regex.search(r'\\d',regex.search(pattern1,j).group())) == True:\n",
    "                try:\n",
    "                    layer = regex.findall(pattern1,j)\n",
    "                    if type(layer) == tuple:\n",
    "                        layer = list(filter(None, [i for i in layer]))\n",
    "                        prepro2.append(re.sub(layer[0],'',j))\n",
    "                    else:\n",
    "                        prepro2.append(re.sub(regex.findall(pattern1,j)[0],'',j))\n",
    "                except:\n",
    "                    prepro2.append(j)\n",
    "            else:\n",
    "                prepro2.append(j)\n",
    "        else:\n",
    "            prepro2.append(j)\n",
    "\n",
    "        #Weird character of ﬂ\n",
    "        layer = regex.sub(r'ﬂ','fl',prepro2[i])\n",
    "\n",
    "        #With Removing number and punctuations    \n",
    "        layer = regex.sub(r'(\\d|[^A-Za-zöäüéíáúóðèñæýßôþ\\s]|[^\\P{P}]+)',' ',re.sub(pattern2,'', layer)) \n",
    "\n",
    "        #With Removing unused space first n las\n",
    "        layer = regex.sub(pattern3,'',layer)\n",
    "\n",
    "        #With Removing over-space\n",
    "        layer = regex.sub(pattern4,'',layer)\n",
    "\n",
    "        #With Removing Additional compiler\n",
    "        prepro2[i] = regex.sub(pattern_add,'',layer)\n",
    "\n",
    "    #Final filtering for less than three words sentence\n",
    "    d = lambda y : None if len(y.split(' '))<=4 else y\n",
    "    prepro2 = list(filter(d, prepro2))\n",
    "    \n",
    "    return prepro2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7b63f",
   "metadata": {},
   "source": [
    "# Function to build NER Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343bfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_entities(folder_path):\n",
    "    dict_entities={}\n",
    "    for filename in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "        gist_file = open(filename, \"r\")\n",
    "        try:\n",
    "            content = gist_file.read().lower()\n",
    "            dict_entities[re.search(r'(\\w+).txt$',filename).group(1)] = list(filter(None, content.split(\"\\n\")))\n",
    "        finally:\n",
    "            gist_file.close()\n",
    "            \n",
    "def create_lib_entities(prepro2):\n",
    "    #Ready for NLP NER\n",
    "    ready_ner = {'text':[],'tag':[],'entity':[]}\n",
    "    for i in prepro2:\n",
    "        for j in dict_entities:\n",
    "            pattern = r\"(\\.*\\b(?:{}))\\b\".format(\"|\".join(dict_entities[j]))\n",
    "            if bool(re.search(pattern, i)) == True:\n",
    "                for k in re.findall(pattern,i):\n",
    "                    if type(k) == tuple:\n",
    "                        k = list(filter(None,[i for i in k]))\n",
    "                        for m in k:\n",
    "                            ready_ner['text'].append(i)\n",
    "                            ready_ner['tag'].append(m)\n",
    "                            ready_ner['entity'].append(re.search(r'list_(\\w+)$',j).group(1))\n",
    "                    else:\n",
    "                        ready_ner['text'].append(i)\n",
    "                        ready_ner['tag'].append(k)\n",
    "                        ready_ner['entity'].append(re.search(r'list_(\\w+)$',j).group(1))\n",
    "            else:\n",
    "                pass\n",
    "    return ready_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad501a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
